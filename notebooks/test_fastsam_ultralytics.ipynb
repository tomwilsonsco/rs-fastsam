{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import FastSAM, SAM, YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79849d4",
   "metadata": {},
   "source": [
    "## Exploring zero shot segmentation models\n",
    "We will look at the zero shot models and how well they segment an 8 bit RGB Sentinel 2 image over agricultural fields.  \n",
    "[Starting with FastSAM](https://docs.ultralytics.com/models/fast-sam/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0549d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastSAM(\"FastSAM-x.pt\")\n",
    "model.info()\n",
    "everything_results = model(\"dogs.jpg\", device=\"cpu\", retina_masks=True, imgsz=512, conf=0.4, iou=0.9)\n",
    "res = everything_results[0]\n",
    "\n",
    "# Original image as np array\n",
    "plot_img = res.orig_img.copy()\n",
    "\n",
    "# Get the masks (N, H, W)\n",
    "masks = res.masks.data.cpu().numpy()\n",
    "\n",
    "# Create an overlay for all masks\n",
    "overlay = plot_img.copy()\n",
    "\n",
    "# Generate a random color for each mask\n",
    "for i, mask in enumerate(masks):\n",
    "    color = np.random.randint(0, 255, (3,), dtype=np.uint8)\n",
    "    colored_mask = np.stack([mask * c for c in color], axis=-1).astype(np.uint8)\n",
    "    overlay = cv2.addWeighted(overlay, 1.0, colored_mask, 0.5, 0)\n",
    "\n",
    "# Plot segmentation masks result\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Segmentation Masks (FastSAM)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57230e3c",
   "metadata": {},
   "source": [
    "## Testing SAM models on a satellite image\n",
    "We have already created an RGB tif of a Sentinel 2 image with just red, green, blue bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4907443",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_img = repo_dir / \"data\" / \"rgb_fast_sam_test.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "img_arr = img.imread(s2_img)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_arr)\n",
    "plt.axis(\"off\")  # Hide axis\n",
    "plt.title(\"Image Display\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f0448",
   "metadata": {},
   "source": [
    "We can define a common function for extracting and viewing segmentation results as contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmentation_contours(results, model_name):\n",
    "    # Get first result (for one image)\n",
    "    res = results[0]\n",
    "    img = res.orig_img.copy()\n",
    "\n",
    "    # Convert to color if grayscale\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Get masks\n",
    "    masks = res.masks.data.cpu().numpy()\n",
    "\n",
    "    # Draw contours on a copy of the original image\n",
    "    outline_img = img.copy()\n",
    "\n",
    "    for mask in masks:\n",
    "        mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "        cv2.drawContours(outline_img, contours, -1, color, thickness=2)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(outline_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Segmentation Outlines {model_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73e23a",
   "metadata": {},
   "source": [
    "We will now run FastSAM on the image as is (zero shot no model fine tuning, prompting, input points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20062e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and run inference\n",
    "model = FastSAM(\"FastSAM-x.pt\")\n",
    "results = model(s2_img, device=\"cpu\", retina_masks=True, imgsz=512, conf=0.3, iou=0.9)\n",
    "\n",
    "plot_segmentation_contours(results, \"FastSAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d49ecc",
   "metadata": {},
   "source": [
    "## MobileSAM\n",
    "MobileSAM takes longer, but produces better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileSAM\n",
    "model = SAM(\"mobile_sam.pt\")\n",
    "\n",
    "results = model(\n",
    "    s2_img,\n",
    "    device=\"cpu\",\n",
    "    imgsz=1024,\n",
    "    conf=0.3,\n",
    "    iou=0.9,\n",
    ")\n",
    "\n",
    "plot_segmentation_contours(results, \"MobileSAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21a533",
   "metadata": {},
   "source": [
    "The [SAM2t](https://docs.ultralytics.com/models/sam-2/#segment-everything) model does not seem to segment many fields, time taken roughly the same as MobileSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM2 tiny\n",
    "model = SAM(\"sam2_t.pt\")\n",
    "\n",
    "results = model(\n",
    "    s2_img,\n",
    "    device=\"cpu\",\n",
    "    imgsz=1024,\n",
    "    conf=0.3,\n",
    "    iou=0.9,\n",
    ")\n",
    "\n",
    "plot_segmentation_contours(results, \"SAM2-t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6972ea",
   "metadata": {},
   "source": [
    "A YOLO model is extremely fast, does not seem to produce results and must need fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "results = model(\"rgb_fast_sam_test.tif\", device=\"cpu\", imgsz=256, conf=0.1, iou=0.5)\n",
    "\n",
    "if results[0].masks:\n",
    "\n",
    "    plot_segmentation_contours(results, \"yolov8n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fde9f1",
   "metadata": {},
   "source": [
    "In conclusion, FastSAM is fast, works on large image, produces quite good results. MobileSAM produces good results, but much slower. SAM2-t does not pick up all areas, might work well with point prompts. YOLO models available from Ultralytics must need fine tuning for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
