{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.warp import Resampling, reproject\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to Python path to access the src module\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.rssam.classify import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db573930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / \"data\"\n",
    "\n",
    "models_dir = Path.cwd().parent / \"clf_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make s1 VH image smaller for repo storage\n",
    "input_path = data_dir / \"s1_vh_only_may2025.tif\"\n",
    "output_path = data_dir / \"output_VH_uint16.tif\"\n",
    "vmin, vmax = -30.0, 0.0\n",
    "\n",
    "with rio.open(input_path) as src:\n",
    "    # copy profile & set up for uint16 output\n",
    "    profile = src.profile.copy()\n",
    "    profile.update(dtype=rio.uint16, nodata=0, compress=\"lzw\", count=src.count)\n",
    "\n",
    "    vh = src.read(masked=True).astype(np.float32)\n",
    "    mask = vh.mask\n",
    "\n",
    "    input_band_names = src.descriptions\n",
    "\n",
    "# clip to [vmin, vmax]\n",
    "vh_clipped = np.clip(vh.data, vmin, vmax)\n",
    "\n",
    "# scale so 1-1000\n",
    "scale_factor = (1000 - 1) / (vmax - vmin)\n",
    "vh_scaled = ((vh_clipped - vmin) * scale_factor + 1).round().astype(np.uint16)\n",
    "\n",
    "# restore no‑data pixels to 0\n",
    "vh_scaled[mask] = 0\n",
    "\n",
    "# write out uint16 GeoTIFF and keep band names\n",
    "with rio.open(output_path, \"w\", **profile) as dst:\n",
    "    dst.write(vh_scaled)\n",
    "    # set each band desc to match input\n",
    "    for idx, name in enumerate(input_band_names, start=1):\n",
    "        if name is not None:\n",
    "            dst.set_band_description(idx, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the S1 VH bands onto the S2 analysis image\n",
    "\n",
    "s2_path = (\n",
    "    data_dir\n",
    "    / \"S2C_20250516_latn563lonw0021_T30VWH_ORB080_20250516122950_compressed_downscaled.tif\"\n",
    ")\n",
    "s1_path = data_dir / \"output_VH_uint16.tif\"\n",
    "output_path = data_dir / \"s2_plus_s1_stack.tif\"\n",
    "\n",
    "# Get S2 metadata\n",
    "with rio.open(s2_path) as src_s2:\n",
    "    s2_meta = src_s2.profile.copy()\n",
    "    s2_transform = src_s2.transform\n",
    "    s2_crs = src_s2.crs\n",
    "    s2_width = src_s2.width\n",
    "    s2_height = src_s2.height\n",
    "    s2_descriptions = src_s2.descriptions\n",
    "    s2_data = src_s2.read()\n",
    "\n",
    "# Prepare an empty array for S1 reprojected into S2 grid\n",
    "with rio.open(s1_path) as src_s1:\n",
    "    s1_count = src_s1.count\n",
    "    s1_dtype = src_s1.dtypes[0]\n",
    "    s1_descriptions = src_s1.descriptions\n",
    "\n",
    "# destination S1 array\n",
    "s1_aligned = np.zeros((s1_count, s2_height, s2_width), dtype=s1_dtype)\n",
    "\n",
    "# Reproject S1 to S2 grid with nearest‐neighbour\n",
    "with rio.open(s1_path) as src_s1:\n",
    "    reproject(\n",
    "        source=rio.band(src_s1, list(range(1, s1_count + 1))),\n",
    "        destination=s1_aligned,\n",
    "        src_transform=src_s1.transform,\n",
    "        src_crs=src_s1.crs,\n",
    "        dst_transform=s2_transform,\n",
    "        dst_crs=s2_crs,\n",
    "        resampling=Resampling.nearest,\n",
    "    )\n",
    "\n",
    "# Stack S2 and now aligned S1\n",
    "stacked = np.vstack([s2_data, s1_aligned])\n",
    "\n",
    "# Update metadata and write out\n",
    "new_meta = s2_meta.copy()\n",
    "new_meta.update({\"count\": stacked.shape[0], \"dtype\": s1_dtype, \"compress\": \"lzw\"})\n",
    "\n",
    "with rio.open(output_path, \"w\", **new_meta) as dst:\n",
    "    dst.write(stacked)\n",
    "    # keep band desc\n",
    "    all_descriptions = list(s2_descriptions) + list(s1_descriptions)\n",
    "    for idx, desc in enumerate(all_descriptions, start=1):\n",
    "        if desc:\n",
    "            dst.set_band_description(idx, desc)\n",
    "\n",
    "print(f\"Written stack to {output_path} (shape {stacked.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f99b42",
   "metadata": {},
   "source": [
    "### Random forest classification\n",
    "#### First test accuracy of predicting 5 classes using K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0399035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Warning: 'partition' will ignore the 'mask' of the MaskedArray.\"\n",
    ")\n",
    "\n",
    "\n",
    "SOURCE_IMAGE = (\n",
    "    data_dir\n",
    "    / \"S2C_20250516_latn563lonw0021_T30VWH_ORB080_20250516122950_compressed_downscaled.tif\"\n",
    ")\n",
    "\n",
    "# Read the classified GeoJSON\n",
    "print(\"Reading master_classified.geojson...\")\n",
    "gdf = gpd.read_file(data_dir / \"master_classified.geojson\")\n",
    "print(f\"Original polygons: {len(gdf)}\")\n",
    "\n",
    "print(gdf.crs)\n",
    "\n",
    "\n",
    "# Drop any invalid geometries and empty polygons\n",
    "print(\"Filtering valid polygons...\")\n",
    "gdf = gdf[gdf.geometry.is_valid]\n",
    "gdf = gdf[~gdf.geometry.is_empty]\n",
    "gdf = gdf[gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(gdf[\"ml_class\"].value_counts().sort_index())\n",
    "\n",
    "# Load the image and extract pixels\n",
    "with rio.open(SOURCE_IMAGE) as src:\n",
    "    img_profile = src.profile\n",
    "    img_transform = src.transform\n",
    "    img_crs = src.crs\n",
    "\n",
    "    # Extract pixels for each polygon\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        try:\n",
    "            # Mask the image with the polygon\n",
    "            geom = [row.geometry]\n",
    "            masked_img, masked_transform = mask(src, geom, crop=True, filled=False)\n",
    "\n",
    "            # Get valid pixels (not masked)\n",
    "            valid_mask = ~masked_img.mask[0]  # First band mask\n",
    "\n",
    "            if np.any(valid_mask):\n",
    "                # Extract pixel values for all bands\n",
    "                pixels = masked_img[:, valid_mask].T\n",
    "\n",
    "                # We're working with the median per polygon\n",
    "                pixels_med = np.median(pixels, axis=0)\n",
    "\n",
    "                # Give pixels labels in gdf ml_class column\n",
    "                label = int(row[\"ml_class\"])\n",
    "\n",
    "                X_list.append(pixels_med.reshape(1, -1))\n",
    "                y_list.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing polygon {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        print(\"No valid pixels extracted!\")\n",
    "        exit()\n",
    "\n",
    "    # Combine all pixels\n",
    "    X = np.vstack(X_list)\n",
    "    X = calculate_indices(X)\n",
    "    y = np.hstack(y_list)\n",
    "\n",
    "print(f\"\\nTotal pixels extracted: {len(X)}\")\n",
    "print(f\"Features (bands): {X.shape[1]}\")\n",
    "print(f\"Class distribution in extracted pixels:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for class_id, count in zip(unique, counts):\n",
    "    print(f\"  Class {class_id}: {count} pixels ({count/len(y)*100:.1f}%)\")\n",
    "    # Implement 5-fold cross-validation\n",
    "\n",
    "    print(\"\\nImplementing 5-fold cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_accuracies = []\n",
    "    cv_predictions = []\n",
    "    cv_true_labels = []\n",
    "    feature_importances = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nFold {fold + 1}/5\")\n",
    "\n",
    "        # Split data for this fold\n",
    "        X_train_fold = X[train_idx]\n",
    "        X_val_fold = X[val_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "\n",
    "        # Train model for this fold\n",
    "        rf_fold = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "\n",
    "        rf_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_fold = rf_fold.predict(X_val_fold)\n",
    "\n",
    "        # Calculate accuracy for this fold\n",
    "        fold_accuracy = accuracy_score(y_val_fold, y_pred_fold)\n",
    "        cv_accuracies.append(fold_accuracy)\n",
    "\n",
    "        # Store predictions and true labels for overall evaluation\n",
    "        cv_predictions.extend(y_pred_fold)\n",
    "        cv_true_labels.extend(y_val_fold)\n",
    "\n",
    "        # Store feature importances\n",
    "        feature_importances.append(rf_fold.feature_importances_)\n",
    "\n",
    "        print(f\"Fold {fold + 1} accuracy: {fold_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate average accuracy and standard deviation\n",
    "    mean_accuracy = np.mean(cv_accuracies)\n",
    "    std_accuracy = np.std(cv_accuracies)\n",
    "\n",
    "    print(f\"\\n5-Fold Cross-Validation Results:\")\n",
    "    print(f\"Mean accuracy: {mean_accuracy:.4f} (+/- {std_accuracy * 2:.4f})\")\n",
    "    print(f\"Individual fold accuracies: {[f'{acc:.4f}' for acc in cv_accuracies]}\")\n",
    "\n",
    "    # Calculate average feature importance across folds\n",
    "    mean_feature_importance = np.mean(feature_importances, axis=0)\n",
    "    feature_names = [f\"Band_{i+1}\" for i in range(X.shape[1])]\n",
    "    importance_df_cv = pd.DataFrame(\n",
    "        {\"Feature\": feature_names, \"Importance\": mean_feature_importance}\n",
    "    ).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "    print(f\"\\nAverage Feature Importance across 5 folds:\")\n",
    "    print(importance_df_cv)\n",
    "\n",
    "    # Overall classification report using all CV predictions\n",
    "    print(f\"\\nOverall Classification Report (5-fold CV):\")\n",
    "    print(classification_report(cv_true_labels, cv_predictions))\n",
    "\n",
    "    # Overall confusion matrix\n",
    "    print(f\"\\nOverall Confusion Matrix (5-fold CV):\")\n",
    "    cm_cv = confusion_matrix(cv_true_labels, cv_predictions)\n",
    "    print(cm_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cd179",
   "metadata": {},
   "source": [
    "#### Train the model on all data and write as pickle to clf_models dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf.fit(X, y)\n",
    "\n",
    "model_path = models_dir / \"random_forest_classifer_20250717.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1332b3d",
   "metadata": {},
   "source": [
    "#### Exploration - box plots per image band for the 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame with X data and y labels for easier plotting\n",
    "df_plot = pd.DataFrame(X)\n",
    "df_plot[\"class\"] = y\n",
    "\n",
    "# Get the number of columns in X\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Create feature names (you can customize these based on your actual feature names)\n",
    "feature_names = [\n",
    "    \"Blue\",\n",
    "    \"Green\",\n",
    "    \"Red\",\n",
    "    \"RE_B6\",\n",
    "    \"NIR_B8\",\n",
    "    \"SWIR1\",\n",
    "    \"SWIR2\",\n",
    "    \"NDVI\",\n",
    "    \"NDWI\",\n",
    "    \"NBR\",\n",
    "    \"EVI2\",\n",
    "    \"NDVI_RE\",\n",
    "]\n",
    "\n",
    "# If more features than names, pad with generic names\n",
    "if n_features > len(feature_names):\n",
    "    feature_names.extend(\n",
    "        [f\"Feature_{i}\" for i in range(len(feature_names), n_features)]\n",
    "    )\n",
    "\n",
    "# Create subplots - adjust the layout based on number of features\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "fig.suptitle(\"Box Plots of Features by Class\", fontsize=16)\n",
    "\n",
    "# Flatten axes array for easier indexing\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create box plots for each feature\n",
    "for i in range(n_features):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Create box plot grouped by class\n",
    "    data_by_class = [df_plot[df_plot[\"class\"] == cls][i] for cls in np.unique(y)]\n",
    "\n",
    "    bp = ax.boxplot(data_by_class, labels=np.unique(y), patch_artist=True)\n",
    "\n",
    "    # Color the boxes\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(np.unique(y))))\n",
    "    for patch, color in zip(bp[\"boxes\"], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    ax.set_title(feature_names[i])\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for i in range(n_features, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_boxplots_by_class.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\"Plot saved to: feature_boxplots_by_class.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f202e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference class description\n",
    "class_desc = {\n",
    "    0: \"Water\",\n",
    "    1: \"Crop / grass\",\n",
    "    2: \"New crop\",\n",
    "    3: \"Bare ground\",\n",
    "    4: \"Mature crop\",\n",
    "    5: \"Forest\",\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
